# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18I1Mm8zfqjQozPsKBRC-Oh_2nyA17CxD
"""

# !pip install flask pandas nltk vaderSentiment spacy scikit-learn pdfplumber
# !python -m spacy download en_core_web_sm

from flask import Flask, request, jsonify
import pandas as pd
import zipfile
import os
from nltk.corpus import stopwords
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import string
import nltk

nltk.download('stopwords')

# Initialize Flask app
app = Flask(__name__)

# Initialize spaCy model for Named Entity Recognition (NER)
nlp = spacy.load("en_core_web_sm")

# Initialize Sentiment Analyzer
sentiment_analyzer = SentimentIntensityAnalyzer()

# Function to calculate sentiment score and normalize it
def overly_positive_score(text):
    sentiment = sentiment_analyzer.polarity_scores(text)
    compound_score = sentiment['compound']
    # Scale the score to a value between 0 and 1
    normalized_score = (compound_score + 1) / 2  # Convert from [-1, 1] to [0, 1]
    return round(normalized_score, 2)  # Return the score rounded to two decimal places

# Function to preprocess text (convert to lowercase, remove punctuation, stopwords, etc.)
def preprocess_text(text):
    if not isinstance(text, str):
        return ""  # Return empty string if input is not a valid string

    # Step 1: Convert to lowercase
    text = text.lower()

    # Step 2: Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Step 3: Remove numbers
    text = re.sub(r'\d+', '', text)

    # Step 4: Tokenization and stopword removal (consider only alphabetic words)
    tokens = text.split()
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

    return ' '.join(filtered_tokens)

# Function to calculate similarity between résumé and recommendation letter
def calculate_similarity(resume_text, recommendation_text):
    vectorizer = TfidfVectorizer(stop_words='english')
    documents = [resume_text, recommendation_text]
    tfidf_matrix = vectorizer.fit_transform(documents)
    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
    return cosine_sim[0][0]

# Function to extract reference letters from zip file (handle nested folders)
def extract_reference_letters(zip_file_path, extract_to='reference_letters'):
    if not os.path.exists(extract_to):
        os.makedirs(extract_to)

    # Extract zip file contents
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

    # Recursively find all .txt files within the extracted folders
    reference_files = []
    for root, dirs, files in os.walk(extract_to):
        for file in files:
            if file.endswith('.txt'):
                reference_files.append(os.path.join(root, file))

    return reference_files

# Function to safely read text files with encoding fallback
def safe_read_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except UnicodeDecodeError:
        try:
            with open(file_path, 'r', encoding='ISO-8859-1') as file:
                return file.read()
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
            return ""

# Function to analyze a single résumé against multiple reference letters
def analyze_resume_with_references(resume_file_path, reference_files):
    # Read and preprocess résumé text
    resume_text = safe_read_file(resume_file_path)
    preprocessed_resume = preprocess_text(resume_text)

    analysis_results = []

    for reference_file in reference_files:
        # Read and preprocess the reference letter text
        reference_text = safe_read_file(reference_file)

        # Preprocess reference letter
        preprocessed_reference = preprocess_text(reference_text)

        # Calculate sentiment score for the reference letter
        sentiment_score = overly_positive_score(preprocessed_reference)

        # Calculate similarity between résumé and reference letter
        similarity_score = calculate_similarity(preprocessed_resume, preprocessed_reference)

        # Store results for each reference letter
        analysis_results.append({
            'Reference_File': reference_file,
            'Sentiment_Score': sentiment_score,
            'Similarity_Score': similarity_score
        })

    return analysis_results

# Endpoint to analyze résumé and reference letters
@app.route('/analyze', methods=['POST'])
def analyze():
    resume_file_path = request.json.get('resume_file_path')
    zip_file_path = request.json.get('zip_file_path')

    # Extract reference letters from the zip file
    reference_files = extract_reference_letters(zip_file_path)

    if not reference_files:
        return jsonify({"error": "No reference letters found in the zip file."}), 400

    # Analyze the résumé against the extracted reference letters
    results = analyze_resume_with_references(resume_file_path, reference_files)

    # Calculate the average sentiment and similarity scores
    avg_sentiment_score = sum(result['Sentiment_Score'] for result in results) / len(results)
    avg_similarity_score = sum(result['Similarity_Score'] for result in results) / len(results)

    # Calculate weighted score
    weighted_score = (0.8 * avg_similarity_score) + (0.2 * avg_sentiment_score)

    return jsonify({
        "Weighted_Score": round(weighted_score, 2),
        "Average_Sentiment_Score": round(avg_sentiment_score, 2),
        "Average_Similarity_Score": round(avg_similarity_score, 2)
    })

# Run the Flask app
if __name__ == '__main__':
    app.run(debug=True)