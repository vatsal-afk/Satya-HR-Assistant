# -*- coding: utf-8 -*-
"""Resume_Score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCe5V6shj6Ke_jsHM_FzOxGDwUqg0AOX
"""

pip install pdfplumber

import pdfplumber
import re
import zipfile
import os
import matplotlib.pyplot as plt
import pandas as pd
import pandas as pd
import spacy

# Load spaCy's English language model
nlp = spacy.load('en_core_web_sm')

def preprocess_text(text):
    # Remove multiple spaces and newlines
    text = re.sub(r'\s+', ' ', text)  # Replace multiple whitespace/newlines with a single space

    # Remove leading and trailing spaces
    text = text.strip()

    # Optionally, remove non-alphanumeric characters (uncomment the following line if needed)
    # text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    return text

def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text

    # Preprocess the extracted text
    cleaned_text = preprocess_text(text)
    return cleaned_text
# Function to read all text from a file
def read_text_file(file_path):
    encodings = ['utf-8', 'ISO-8859-1', 'latin1', 'cp1252']  # Try multiple encodings
    for enc in encodings:
        try:
            with open(file_path, 'r', encoding=enc) as file:
                return file.read()
        except UnicodeDecodeError:
            continue  # Try the next encoding if this one fails


    # If all encodings fail, read the file in binary mode and replace problematic characters
    with open(file_path, 'rb') as file:
        return file.read().decode('utf-8', errors='replace')
def extract_recommendation_letters(zip_path, extract_dir="recommendation_letters"):
    texts = []
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)

        # Extract text from all PDF files in the extracted directory
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if file.endswith('.txt'):
                    file_path = os.path.join(root, file)
                    texts.append(read_text_file(file_path))
    except Exception as e:
        print(f"Error unzipping or extracting recommendation letters: {e}")
# Function to extract number of jobs and GPA from resume text
def extract_job_count(resume_text):
    # Extract GPA in the formats '4.0 GPA' or 'GPA: 3.0'


    # Count the number of jobs mentioned using regex for "Company Name"
    job_pattern = r'(?i)(Company Name.*?)(?=,\s*|\n|$)'  # Match 'Company Name' followed by any characters until a comma or newline
    job_titles = re.findall(job_pattern, resume_text)
    num_jobs = len(job_titles)  # Count the number of matches

    return num_jobs

# Your existing DataFrame with resumes

# Function to count adjectives in the recommended text
def count_adjectives(text):
    # Check if the text is not NaN or missing
    if pd.isnull(text):
        return 0

    # Process the text using spaCy
    doc = nlp(str(text))  # Ensure the text is a string

    # Count adjectives by checking the POS tag
    adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')

    return adj_count
degree_keywords = [
    r'\bBachelor\b', r"\bBachelor's\b", r'\bBachelors\b',
    r'\bMaster\b', r"\bMaster's\b", r'\bMasters\b'
]

# Function to count the number of degrees based on keywords
def count_degrees(text):
    # Check if the text is not NaN or missing
    if pd.isnull(text):
        return 0

    # Count the number of occurrences of degree-related keywords
    degree_count = sum(len(re.findall(keyword, text, re.IGNORECASE)) for keyword in degree_keywords)

    return degree_count
# List of degree-related keywords to look for
certificate_keywords = [
    r'\bCertificate\b',r'\bCertified\b'

]

# Function to count the number of degrees based on keywords
def count_certificate(text):
    # Check if the text is not NaN or missing
    if pd.isnull(text):
        return 0

    # Count the number of occurrences of degree-related keywords
    certificate_count = sum(len(re.findall(keyword, text, re.IGNORECASE)) for keyword in certificate_keywords)

    return certificate_count
# Expanded list of soft skills
soft_skills = [
    "communication", "teamwork", "leadership", "creativity", "work ethic", "adaptability",
    "problem solving", "time management", "critical thinking", "initiative", "interpersonal skills",
    "dependability", "motivation", "collaboration", "decision-making", "negotiation", "organization",
    "active listening", "empathy", "flexibility"
]

# Expanded list of vague terms


# Expanded list of technical skills
# Expanded list of technical skills
technical_skills = [
    "python", "java", "c++", "javascript", "html", "css", "sql", "php", "ruby", "R", "matlab",
    "swift", "kotlin", "objective-c", "typescript", "shell scripting", "perl", "bash", "powershell",
    "c#", "Go", "rust", "scala", "dart", "vba", "visual basic", "assembly language", "xml", "json",
    "nosql", "mongodb", "postgresql", "mysql", "oracle database", "redis", "sqlite", "apache cassandra",
    "elasticsearch", "firebase", "couchbase", "hadoop", "apache spark", "apache kafka", "tensorflow",
    "pytorch", "keras", "scikit-learn", "opencv", "numpy", "pandas", "matplotlib", "seaborn", "tableau",
    "power bi", "microsoft excel", "google sheets", "d3.js", "plotly", "jupyter notebooks", "git",
    "github", "bitbucket", "gitlab", "mercurial", "subversion", "docker", "kubernetes", "jenkins",
    "ansible", "chef", "puppet", "terraform", "vagrant", "nagios", "zabbix", "prometheus", "grafana",
    "elastic stack", "aws", "microsoft azure", "google cloud platform", "ibm cloud", "heroku", "netlify",
    "digitalocean", "cloudfoundry", "openstack", "nginx", "apache http server", "iis", "weblogic", "tomcat",
    "rabbitmq", "activemq", "websockets", "rest apis", "graphql", "soap", "oauth", "openid connect",
    "json web tokens", "openssl", "tls", "saml", "ldap", "sso", "load balancing", "firewall configuration",
    "vpn", "ssh", "cybersecurity", "network security", "intrusion detection systems", "intrusion prevention systems",
    "endpoint security", "penetration testing", "ethical hacking", "vulnerability scanning", "owasp", "encryption",
    "pki", "siem", "ids/ips systems", "data encryption", "ssl certificates", "agile", "scrum", "kanban",
    "lean", "devops", "ci/cd", "sdlc", "tdd", "bdd", "uml", "jira", "trello", "asana", "basecamp", "confluence",
    "microsoft project", "monday.com", "sap", "salesforce", "zoho crm", "hubspot crm", "workday", "peoplesoft",
    "bamboohr", "adp", "servicenow", "zendesk", "freshdesk", "pagerduty", "salesforce lightning",
    "oracle e-business suite", "sap hana", "tableau server", "power bi service", "microsoft dynamics",
    "sage", "quickbooks", "hyper-v", "vmware", "citrix", "virtualbox", "parallels", "windows server",
    "linux", "unix", "bash scripting", "active directory", "exchange server", "sharepoint", "office 365",
    "g suite", "android development", "ios development", "react native", "xamarin", "flutter", "ionic",
    "phonegap", "unity3d", "unreal engine", "blender", "maya", "autocad", "solidworks", "figma",
    "adobe photoshop", "adobe illustrator", "adobe xd", "sketch", "canva", "invision","Patient Assessment",
    "Electronic Health Records (EHR)",
    "Medical Coding (ICD-10, CPT)",
    "Telemedicine",
    "Diagnostic Imaging (X-rays, MRIs, CT Scans)",
    "Triage Skills",
    "Vital Signs Monitoring",
    "Phlebotomy",
    "CPR and First Aid",
    "Wound Care",
    "Medication Administration",
    "Surgical Assisting",
    "IV Insertion and Management",
    "Infection Control",
    "Rehabilitation Techniques",
    "Radiology Techniques",
    "Clinical Research",
    "Health Informatics",
    "Pathology Testing",
    "Laboratory Skills",
    "Physical Therapy Techniques",
    "Emergency Response (ER Procedures)",
    "Healthcare Data Analysis",
    "Chronic Disease Management",
    "Health Education and Counseling","Personal Training",
    "Strength Training",
    "Cardiovascular Fitness",
    "Group Exercise Instruction",
    "Functional Fitness Training",
    "Exercise Physiology",
    "Sports Nutrition",
    "Weight Management",
    "Pilates Instruction",
    "Yoga Instruction",
    "Kinesiology",
    "Flexibility Training",
    "Biomechanics",
    "Aerobics Instruction",
    "Endurance Training",
    "Sports Conditioning",
    "High-Intensity Interval Training (HIIT)",
    "Fitness Assessment",
    "Sports Injury Management",
    "Body Composition Analysis","Mindfulness Training",
    "Stress Management Techniques",
    "Meditation Guidance",
    "Cognitive Behavioral Therapy (CBT)",
    "Emotional Intelligence Coaching",
    "Mental Health Counseling",
    "Holistic Health Practices",
    "Wellness Coaching",
    "Nutrition Planning and Counseling",
    "Lifestyle Medicine",
    "Sleep Therapy Techniques",
    "Life Coaching",
    "Aromatherapy",
    "Reiki Healing",
    "Art Therapy",
    "Occupational Therapy Techniques",
    "Addiction Counseling",
    "Positive Psychology Techniques",
    "Behavior Change Strategies",
    "Psychotherapy Techniques"
]# Function to detect soft skills mentioned without examples
def detect_soft_skills(text):
    text = text.lower()
    skills_found = [skill for skill in soft_skills if skill in text]
    return len(skills_found)

# Function to detect technical skills
def detect_technical_skills(text):
    text = text.lower()
    tech_skills_found = [skill for skill in technical_skills if skill in text]
    return len(tech_skills_found)
def calculate_experience(text):
    """
    Extracts years from the given text and calculates the experience in years.

    Parameters:
        text (str): The input text containing dates or years.

    Returns:
        int: The calculated experience in years, or None if there are not enough years.
    """
    year_pattern = r'\b(?:\d{1,2}/)?(19|20)\d{2}\b'

    # Use regex to find all years in the text
    dates = [match.group() for match in re.finditer(year_pattern, text)]
    years = []

    for date in dates:
        # Check if the format is MM/YYYY
        if re.match(r'\b\d{1,2}/(19|20)\d{2}\b', date):
            year = date.split('/')[1]  # Get the year part after the slash
            years.append(year)
        # Check if it's a standalone year
        elif re.match(r'\b(19|20)\d{2}\b', date):
            years.append(date)  # Directly add the standalone year

    if len(years) < 2:
        return None  # Not enough years to calculate experience

    # Calculate experience
    experience = int(max(years)) - int(min(years))
    return experience


def calculate_resume_score(resume_text, recommendation_texts):
    # Example: Calculate score based on total word count



    df = pd.DataFrame([{
    'Resume_Text': resume_text,
    'Recommendation_Text': recommendation_texts
    }])
    df['Number_of_Jobs'] = df['Resume_Text'].apply(extract_job_count)
    df['Number_of_Adjectives'] = df['Recommendation_Text'].apply(count_adjectives)
    df['Number_of_Degrees'] = df['Resume_Text'].apply(count_degrees)
    df['Number_of_Certificates'] = df['Resume_Text'].apply(count_certificate)
    df['Technical skill']=df['Resume_Text'].apply(detect_technical_skills)
    df['Soft_skill']=df['Resume_Text'].apply(detect_soft_skills)
    df['Years_of_Experience']=df['Resume_Text'].apply(calculate_experience)



    # prompt: now i want assign a certain weightage to each parameter except id and want to calculate score of each id

# Define weightage for each parameter (excluding 'ID')

    weightage= {
      'Years_of_Experience': 0.3,  # Highest weight
      'Number_of_Degrees': 0.2,
      'Number_of_Adjectives': 0.15,
      'Technical skill': 0.12,
      'Soft_skill': 0.1,
      'Number_of_Jobs': 0.08,
      'Number_of_Certificates': 0.05,
    }

# Create a new column for the calculated score
    df['Score'] = 0

# Calculate the score for each ID based on weightage
    for index, row in df.iterrows():
      score = 0
      for column, weight in weightage.items():
        score += row[column] * weight
    df.loc[index, 'Score'] = score

# Print the DataFrame with the calculated scores
    resume_score = df['Score'].sum()


# Save the updated DataFrame with the calculated score
# new_df.to_csv('resume_dataset_with_score.csv', index=False)






    return resume_score

# Function to generate a bar plot comparing resume and recommendation scores


# Main function to orchestrate the process
def main():
    # Input file paths
    resume_pdf_path = "/content/sample resume.pdf"  # Replace with the actual resume file path
    recommendation_zip_path = "/content/sample recommendation.zip"  # Replace with actual zip file path

    # Step 1: Extract text from the resume PDF
    resume_text = extract_text_from_pdf(resume_pdf_path)

    # Step 2: Extract text from the recommendation letters (unzipped and read from PDFs)
    recommendation_texts = extract_recommendation_letters(recommendation_zip_path)

    # Step 3: Calculate the resume score and keyword counts
    resume_score = calculate_resume_score(resume_text, recommendation_texts)

    # Output scores
    print(f"Resume score: {resume_score}")



if __name__ == "__main__":
    main()

